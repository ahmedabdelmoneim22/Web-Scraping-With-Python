{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7db72373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeautifulSoup + Requests | Web Scraping in Python.\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"BeautifulSoup + Requests | Web Scraping in Python.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01d3b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hacker News Web Scarping With Python\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Hacker News Web Scarping With Python\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3f38a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 707, 61, 281, 70, 209, 316, 10, 24, 6, 185, 22, 42, 32, 111, 19, 184, 75, 162, 31, 562, 53, 15, 259, 8, 41, 231, 235, 29]\n",
      "707\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://news.ycombinator.com\")\n",
    "\n",
    "yc_web_page = response.text\n",
    "#Prettify()>>It's used to make the HTML or XML content more Readable.\n",
    "soup = BeautifulSoup(yc_web_page, 'html.parser')\n",
    "soup2 = BeautifulSoup(soup.prettify(), 'html.parser')\n",
    "#print(soup2)#>>Print All Data in Web Page.\n",
    "#News>>Title>>Rightclick>>Inspect Elements.\n",
    "#<span class=\"titleline\"><a href=\"https://www.esa.int/Science_Exploration/Space_Science/Webb/\n",
    "#Webb_captures_iconic_Horsehead_Nebula_in_unprecedented_detail\">Webb captures iconic Horsehead Nebula in unprecedented detail</a>\n",
    "#<span class=\"sitebit comhead\"> (<a href=\"from?site=esa.int\"><span class=\"sitestr\">esa.int</span></a>)</span></span>\n",
    "#All_Titles = soup2.find_all(name=\"span\",class_=\"titleline\")\n",
    "article_titles = []\n",
    "article_links = []\n",
    "for article_tag in soup.find_all(name=\"span\", class_=\"titleline\"):\n",
    "    article_titles.append(article_tag.getText())# Add Article To List.\n",
    "    article_links.append(article_tag.find(\"a\")[\"href\"])# Add Links to List.\n",
    "#print(article_titles)\n",
    "#print(article_links)\n",
    "#print(soup2.find_all(name=\"span\",class_=\"score\"))\n",
    "\"\"\"\n",
    "article_upvotes = []\n",
    "for article in soup.find_all(name=\"td\", class_=\"subtext\"):\n",
    "    if article.span.find(class_=\"score\") is None:\n",
    "        article_upvotes.append(0)\n",
    "    else:\n",
    "        article_upvotes.append(int(article.span.find(class_=\"score\").contents[0].split()[0]))\n",
    "\n",
    "max_points_index = article_upvotes.index(max(article_upvotes))\n",
    "print(\n",
    "    f\"{article_titles[max_points_index]}, \"\n",
    "    f\"{article_upvotes[max_points_index]} points, \"\n",
    "    f\"available at: {article_links[max_points_index]}.\"\n",
    ")\n",
    "\"\"\"\n",
    "score_list = []\n",
    "for score in soup2.find_all(name=\"span\",class_=\"score\"):\n",
    "    #print(int(score.getText().split()[0]))\n",
    "    score_list.append(int(score.getText().split()[0]))\n",
    "print(score_list)\n",
    "print(max(score_list))\n",
    "print(score_list.index(max(score_list)))\n",
    "max_points_index = score_list.index(max(score_list))\n",
    "print(max_points_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a657c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Article Title: Webb captures iconic Horsehead Nebula in unprecedented detail (esa.int)\n",
      "The Article Score: 707\n",
      "The Article Link : https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_captures_iconic_Horsehead_Nebula_in_unprecedented_detail\n"
     ]
    }
   ],
   "source": [
    "print(\"The Article Title:\",article_titles[max_points_index])\n",
    "print(\"The Article Score:\",score_list[max_points_index])\n",
    "print(\"The Article Link :\",article_links[max_points_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "854f995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 Greatest Movies Web Scarping With Python\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"The 100 Greatest Movies Web Scarping With Python\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6385058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      "          1) The Godfather\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "The_100_Greatest = []\n",
    "response = requests.get(\"https://web.archive.org/web/20200518073855/https://www.empireonline.com/movies/features/best-movies-2/\")\n",
    "print(response)\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")#-Create-BeautifulSoup.\n",
    "#print(soup)\n",
    "soup2 = BeautifulSoup(soup.prettify(),'html.parser')\n",
    "#print(soup2)\n",
    "#print(soup2.find_all(name = \"h3\",class_ = \"title\"))\n",
    "for title in soup2.find_all(name = \"h3\",class_ = \"title\"):\n",
    "    The_100_Greatest.append(title.getText())\n",
    "The_100_Greatest.reverse()\n",
    "print(The_100_Greatest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c9255e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for film in The_100_Greatest:\n",
    "    with open(\"TheGreatestMoves.txt\",\"a\",encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{film}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb49b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billboard Hot 100™ spotipy Web Scraping With Python\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Billboard Hot 100™ spotipy Web Scraping With Python\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f238e77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>>Connecting-with-the-Spotify-API.\\nBrowse to https://developer.spotify.com/dashboard/applications.\\nLog in with your Spotify account.\\nClick on ‘Create an app’.\\nPick an ‘App name’ and ‘App description’ of your choice and mark the checkboxes.\\nAfter creation, you see your ‘Client Id’ and you can click on ‘Show client secret` to unhide your ’Client secret’.\\nDashboard>>Create App.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\">>Connecting-with-the-Spotify-API.\n",
    "Browse to https://developer.spotify.com/dashboard/applications.\n",
    "Log in with your Spotify account.\n",
    "Click on ‘Create an app’.\n",
    "Pick an ‘App name’ and ‘App description’ of your choice and mark the checkboxes.\n",
    "After creation, you see your ‘Client Id’ and you can click on ‘Show client secret` to unhide your ’Client secret’.\n",
    "Dashboard>>Create App.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9402193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spotipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from spotipy) (1.26.11)\n",
      "Requirement already satisfied: requests>=2.25.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from spotipy) (2.28.1)\n",
      "Requirement already satisfied: redis>=3.5.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from spotipy) (5.0.4)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from spotipy) (1.16.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from redis>=3.5.3->spotipy) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests>=2.25.0->spotipy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests>=2.25.0->spotipy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests>=2.25.0->spotipy) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install spotipy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1678ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which year do you want to travel to? Type the date in this format YYYY-MM-DD: 2000-11-11\n",
      "['With Arms Wide Open', 'Music', 'Kryptonite', 'Come On Over Baby (All I Want Is You)', 'Independent Women Part I', 'Most Girls', 'This I Promise You', 'Gotta Tell You', 'Case Of The Ex (Whatcha Gonna Do)', \"Jumpin', Jumpin'\", 'The Way You Love Me', 'Shape Of My Heart', 'Shake Ya Ass', 'Between Me And You', 'Bag Lady', 'She Bangs', 'Incomplete', 'No More', '(Hot S**t) Country Grammar', 'Bent', 'Dance With Me', 'I Wish', \"You're A God\", 'Liar', 'Pinch Me', 'E.I.', \"Don't Think I'm Not\", 'Best Of Intentions', 'Give Me Just One Night (Una Noche)', \"If You're Gone\", 'Faded', 'He Loves U Not', \"What's Your Fantasy\", \"He Wasn't Man Enough\", 'I Just Wanna Love U (Give It 2 Me)', 'Wonderful', \"Doesn't Really Matter\", 'The Little Girl', \"It Wasn't Me\", 'Bounce With Me', 'I Need You', 'Back Here', 'Just Another Day In Paradise', \"It's My Life\", 'We Danced', 'Go On', 'Higher', '911', 'Without You', 'Crazy For This Girl', 'Who Let The Dogs Out', 'My Next Thirty Years', 'Feels Like Love', 'Just Friends (Sunny)', 'Ms. Jackson', 'Girls Dem Sugar', 'Born To Fly', 'I Lost It', 'www.memory', 'Kiss This', 'Beautiful Day', 'My First Love', 'There You Are', 'Pop Ya Collar', 'That Other Woman', 'Last Resort', 'Stan', 'Open My Heart', 'Loser', 'Hemorrhage (In My Hands)', 'Californication', 'Just Be A Man About It', \"That's The Kind Of Mood I'm In\", 'Sleepwalker', \"You Should've Told Me\", 'My Baby You', \"Rollin'\", 'Bad Boyz', \"Can't Fight The Moonlight\", \"You Won't Be Lonely Now\", 'I Will...But', 'Summer Rain', \"Aaron's Party (Come Get It)\", 'What Means The World To You', \"No More (Baby I'ma Do Right)\", 'The Light', 'My Love Goes On And On', 'Hey Papi', 'Treat Her Like A Lady', \"Don't Call Me Baby\", \"Let's Make Love\", 'Do You', 'Country Comes To Town', 'Oklahoma', 'Deep Inside Of You', 'Where I Wanna Be', 'Straight Up', 'Change Your Mind', \"It's Always Somethin'\", 'What You Want']\n"
     ]
    }
   ],
   "source": [
    "# Scraping Billboard 100\n",
    "date = input(\"Which year do you want to travel to? Type the date in this format YYYY-MM-DD: \")\n",
    "response = requests.get(\"https://www.billboard.com/charts/hot-100/\" + date)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#song_names_spans = soup.select(\"li ul li h3\")\n",
    "soup2 = BeautifulSoup(soup.prettify(),'html.parser')\n",
    "song_names_spans = soup2.select(\"li ul li h3\")\n",
    "#print(song_names_spans)\n",
    "song_names = [song.getText().strip() for song in song_names_spans]\n",
    "print(song_names)\n",
    "#Client_ID = \"da600680b07348cb87b2decea29046fe\"\n",
    "#Client_Secret=\"0bbebc55b79241cca4b39649584c12ff\"\n",
    "##Spotify Authentication;\n",
    "#sp = spotipy.Spotify(\n",
    "#    auth_manager=SpotifyOAuth(\n",
    "#        scope=\"playlist-modify-private\",\n",
    "#        redirect_uri=\"http://example.com\",\n",
    "#        client_id=Client_ID,\n",
    "#        client_secret=Client_Secret,\n",
    "#        show_dialog=True,\n",
    "#        cache_path=\"token.txt\"\n",
    "#    )\n",
    "#)\n",
    "#user_id = sp.current_user()[\"id\"]\n",
    "##playlist= sp.user_playlist_create(user_id,\"Funky Music\",public=False,collaborative=False, description=\"Test\")\n",
    "#print(user_id)\n",
    "#Searching Spotify for songs by title\n",
    "#song_uris = []\n",
    "#year = date.split(\"-\")[0]\n",
    "#for song in song_names:\n",
    "#    result = sp.search(q=f\"track:{song} year:{year}\", type=\"track\")\n",
    "#    print(result)\n",
    "#    try:\n",
    "#        uri = result[\"tracks\"][\"items\"][0][\"uri\"]\n",
    "#        song_uris.append(uri)\n",
    "#    except IndexError:\n",
    "#        print(f\"{song} doesn't exist in Spotify. Skipped.\")\n",
    "#Creating a new private playlist in Spotify\n",
    "#playlist = sp.user_playlist_create(user=user_id, name=f\"{date} Billboard 100\", public=False)\n",
    "#print(playlist)\n",
    "#Adding songs found into the new playlist\n",
    "#sp.playlist_add_items(playlist_id=playlist[\"id\"], items=song_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BAI chat.\n",
    "Create an Automated Amazon Price Tracker. With a Python.\n",
    "Sure! I can help you create an automated Amazon price tracker using Python.\n",
    "Here's a sample code to get you started:\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import smtplib\n",
    "\n",
    "# Set the URL of the product you want to track\n",
    "url = ''\n",
    "\n",
    "# Set your target price (the price at which you want to be notified)\n",
    "target_price = 50\n",
    "\n",
    "# Set your email details\n",
    "sender_email = 'your-email@example.com'\n",
    "sender_password = 'your-email-password'\n",
    "receiver_email = 'recipient-email@example.com'\n",
    "\n",
    "def check_price():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    # Send a request to Amazon's servers and retrieve the HTML of the page\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the title and price of the product\n",
    "    title = soup.find(id='productTitle').get_text().strip()\n",
    "    price = float(soup.find(id='priceblock_ourprice').get_text().replace('$', '').replace(',', ''))\n",
    "    \n",
    "    # Check if the current price is lower than the target price\n",
    "    if price < target_price:\n",
    "        send_email(title, price)\n",
    "\n",
    "def send_email(title, price):\n",
    "    subject = f'Price Drop Alert: {title}'\n",
    "    body = f\"The price of the product '{title}' has dropped below ${target_price}. It is now ${price}.\\n\\nYou can check it here: {url}\"\n",
    "    message = f\"Subject: {subject}\\n\\n{body}\"\n",
    "    \n",
    "    with smtplib.SMTP('smtp.gmail.com', 587) as server:\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "        server.sendmail(sender_email, receiver_email, message)\n",
    "\n",
    "check_price()\n",
    "\"\"\"\n",
    "############$$$$$$$$$$$$$$$$$$$$$$$$$$#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041db53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The-End\n"
     ]
    }
   ],
   "source": [
    "print(\"The-End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83dcee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
